= Trusted Software Supply Chain with {product_rhdh_name} for Production

== What is 'end product'?

So far in this course we have looked at the individual components of the composite solution. This module will explain how they all work together, and more importantly why this is a crucial product set for end customers.

In order to understand this we need to look at the concept of 'End Product'. This is the deliverable from one part of an organisation to another; the value in motion. It used to vary widely; some organisations wanted developers to deliver code, some binaries, some composite packages. In the new world of DevSecOps and the advent of the inner and outer loops, we now have a defined point of delivery for developers - code in a Git repository.

With the separation of the inner loop — where developers handle code generation, functional testing, and delivery via Git — and the outer loop — where Ops and Platform Engineers manage builds, functional and non-functional testing, and deployment — it became critical to apply and assess security much earlier in the software lifecycle.

In the old days of https://www.atlassian.com/agile/project-management/waterfall-methodology[Waterfall^], security was usually the last thing to be checked - if at all. This meant that it was at the point of functional/non-functional testing that security checks were applied. If they failed, it was back to the beginning, with the developers having to produce code which went through the entire lifecycle again. 

The concept of https://www.redhat.com/en/topics/devops/shift-left-vs-shift-right[shift-left^] brings security much earlier in the software development cycle. {product_rhdh_name} (RHDH) and {product_tssc_name} (TSSC) provide secure tooling to allow developers to see potential security issues as they type their code, with TSSC detecting and recording potential security issues as part of the standard build approach. 

How this works from the product perspective is as follows: RHDH provides the configurable Internal Developer Portal (IDP), including direct linkage to in-browser editors, via OpenShift Dev Spaces. {product_rhads_name} (RHADS) provides extensions for these in-browser editors to highlight potential CVE exploit points, so a developer can directly see the issues as they develop. 

TSSC provides pre-built Tekton pipelines (for continuous integration) that transform source code into container image, and apply a set of security protocols to it as part of the build process. This locks the generation of 'end-product', in this case the application container image, into a "security-gate" approach, where if any of the checks fail, the build fails. 

In addition, TSSC generates SBOMs (Software Bill of Material) which record exactly what components were used to create the digital artifacts. These SBOMs are also now part of the 'end-product', and are generated and maintained as part of the build pipeline. 

Finally, to make it all seamless, templates can be used for application scaffolding, to generate a code repository, webhooks (so *any* update to the code repository causes the TSSC pipeline to start), and generates all the deployable components for the Application on OpenShift using ArgoCD.

This last point is very important; rather than interact directly with the cluster from the scaffolding perspective, all components/objects are applied via GitOps. This allows the template, executed in RHDH, to scaffold not only the developer environment (the Git repositories and in-browser editors) but the TSSC pipelines *and* the end deployment components. 

The entire process, from the point at which the developer submits the code ('end-product') to final deployment post-build and security check, is fully automated.

== Why do we need RHDH and TSSC?

As it stands, TSSC provides the pre-defined Tekton pipelines for performing all of the checks and build components to take a piece of source and generate a useable Image along with all the receipts (SBOMs) in the case of a successful build, or pinpoint log information as to why the build failed or was stopped by security. However these are just a set of Tekton pipeline definitions; you need to be able to run them (i.e. OpenShift). 

With the advent of {product_tpa_name} (TPA), the SBOM generator and maintainer, and {product_as_name} (TAS), which provides cryptographic signing and verification, the combination of TSSC, TAS and TPA provides the whole secure process, but again you need to parameterise it appropriately.

With RHDH you have the capability of writing templates which walk the user through a sequential set of steps in order to scaffold an environment. What we have done with RHADS is to build an opinionated approach, through templates in RHDH, for using TSSC and the other components. This approach makes it much easier for a Platform Engineer to setup the process, and much easier (with way less cognitive load) for a Developer to use. In fact a Developer will be largely insulated from the process, which makes a Developer's life easier. 

== Positioning the technical components

From a technical sales perspective, {product_rhads_name} is an easy pitch because it delivers clear value to two key personas:

* For Developers: They gain a simplified way to write software without getting bogged down in the minutiae of the processes for getting started, building, and delivering the end product. This is precisely where Backstage/RHDH is positioned—to provide a smooth, self-service experience.

* For Operations (Ops) Teams: They get secured software that has been rigorously tested for potential exploits before it reaches the final functional and non-functional testing phases. This directly addresses the "Shift-Left" need for early, enforced security.

Put simply, TSSC, TAS, and TPA give you automated build and security scanning, point of truth (SBOM generation and maintenance) and the stamp of security approval (cryptographic signing).

Most customers are doing these things already but the diversity of solutions means it is a headache to maintain these build factories/pipelines. RHADS, through the developer facing components of RHDH and the Ops facing components of end-product generation with TSSC, TAS and TPA, solves a lot of this complexity and provides clear and defined interaction points; the developers work with templates (which can be extended if the organisation needs additional steps for scaffolding), the Ops/Platform Engineers work by maintaining the template and pre-generated Tekton pipelines from TSSC. 

For the majority of organisations, the out-of-the-box functionality provided by the templates and the standard TSSC pipelines will suffice to provide a solid secure development/build lifecycle. 

== Understanding the process

=== Scaffolding with templates and the TSSC pipelines

The key component in all of this approach is the RHDH template. This is a configurable object that defines a set of actions to be executed sequentially; if all actions complete successfully, the template is deemed to have completed. If any actions fail, the template stops at that point.

It is worth understanding what is meant by an action in a template; as you have seen in previous modules, the framework of Backstage and therefore RHDH provides massive extensibility using the concepts of 'plugins'. These provide multiple forms of extension for the framework, visual components that can be rendered as part of the composite portal provided to the end consumer, API endpoints that hosted by the developer portal, and actions that can be performed by templates. These actions are the powerful parts for us.

Let’s look at an example template to show how they are used - this is the template will we execute as part of the hands-on lab:

```yaml
apiVersion: scaffolder.backstage.io/v1beta3
kind: Template
metadata:
  name: quarkus-stssc-template
  title: Securing a Quarkus Service Software Supply Chain (Tekton)
  description: Create a Quarkus Service built with Red Hat Trusted Application Pipeline on Tekton
  tags:
    - recommended
    - java
    - quarkus
    - maven
```

The template starts with metadata that is used within the framework for display and selection. The tags are actually rendered on the selection tile in the provisioned portal. As with all objects, the definition is split into the metadata, shown above, and the specification of the object, in this case the template definition. 

```yaml
spec:
  owner: tssc
  type: service
```

The specification starts with some core definition fields, in this case the owner object and the type of object produced by the template. Now we have an extract of the parameters section of the specification:

```yaml
  parameters:
    - title: Provide Information for Application
      required:
        - name
        - javaPackageName
      properties:
        name:
          title: Name
          type: string
          description: Unique name of the component
          default: my-quarkus-tkn
          ui:field: EntityNamePicker
          maxLength: 23
        groupId:
          title: Group Id
          type: string
          default: redhat.rhdh
          description: Maven Group Id
        artifactId:
          title: Artifact Id
          type: string
          default: my-quarkus-tkn
          description: Maven Artifact Id
        javaPackageName:
          title: Java Package Name
          default: org.redhat.rhdh
          type: string
          description: Name for the java package. eg (com.redhat.blah)
        description:
          title: Description
          type: string
          description: Help others understand what this website is for.
          default: A cool quarkus app
    - title: Provide Image Registry Information
      required:
        - imageHost
        - imageOrganization
      properties:
        imageHost:
          title: Image Registry
          type: string
          default: Quay
          enum:
            - Quay
        imageOrganization:
          title: Organization
          type: string
          description: Name of the Quay Organization
          default: tssc
```
When you, as a user of the portal generated by RHDH, instantiate a template, the framework parses all of the parameters required in the specification; these are rendered as wizards, with each `title:` group being rendered as a separate page in an overall form. This is an extract, but note the first 'form', 'Provide Information for Application'. This has the defined parameters name, groupID etc - each parameter can be defined to be optional or mandatory, and a default value can be provided. As we will see in the steps defined next in the template, these parameters can be passed into the action calls in each step. Here’s an example couple of steps:

```yaml
steps:
    - id: fetch-provision-data
      name: Fetch Provision Data
      action: catalog:fetch
      input:
        entityRef: component:default/provisioning-data

    - id: template
      name: Fetch Skeleton + Template
      action: fetch:template
      input:
        url: ./skeleton
        values:
          name: ${{ parameters.name }}
          namespace: tssc-app
          description: ${{ parameters.description }}
          groupId: ${{ parameters.groupId }}
          artifactId: ${{ parameters.artifactId }}
          javaPackageName: ${{ parameters.javaPackageName }}
          owner: user:default/${{ user.entity.metadata.name }}
          cluster: ${{ steps["fetch-provision-data"].output.entity.metadata.labels["ocp-apps-domain"] }}
          gitlabHost: gitlab-gitlab.${{ steps["fetch-provision-data"].output.entity.metadata.labels["ocp-apps-domain"] }}
          quayHost: quay-${{ steps["fetch-provision-data"].output.entity.metadata.labels["guid"] }}.${{ steps["fetch-provision-data"].output.entity.metadata.labels["ocp-apps-domain"] }}
          destination: ${{ parameters.repoOwner }}/${{ parameters.name }}
          quayDestination: ${{ parameters.imageOrganization}}/${{ parameters.name }}
          port: 8080
          verifyCommits: ${{ parameters.repoVerifyCommits }}
```
Firstly note that every step has an `action:` field. This refers to either a built in action (the `fetch:template` and `catalog:fetch` are pre-built actions in the core Backstage framework) or an action provided by a plugin. In the hands-on lab we will see that the next step is actually `publish:gitlab`, which is an action to push the generated files to GitLab. This functionality is provided in the https://backstage.io/docs/reference/plugin-scaffolder-backend-module-gitlab/#functions[appropriate plugin^] added to the core framework.

At the end of the template are a set of 'outputs'. These are rendered components on the portal that can link to entities created by the template, or display links and text of your choosing:

```yaml
output:
    links:
      - title: Source Repository
        url: ${{ steps['publish-gitlab-source'].output.remoteUrl }}
      - title: GitOps Repository
        url: ${{ steps['publish-gitlab-gitops'].output.remoteUrl }}
      - title: Open Component in catalog
        icon: catalog
        entityRef: ${{ steps['register-source'].output.entityRef }}
      - title: Open GitOps Resource in catalog
        icon: catalog
        entityRef: ${{ steps['register-gitops'].output.entityRef }}
```
Note that like the parameter variables, the Backstage scaffolder will also expose variables from the output of the steps. Everything bound by double curly-braces is considered an expression. For example, `${{ steps['publish-gitlab-source'].output.remoteUrl }}`, which is the URL that is created for the repo that is scaffolded by one of the steps will be evaluated and replaced by the actual value. You can read more about these https://backstage.io/docs/features/software-templates/writing-templates/#more-about-expressions/[Nunjucks based templates in the docs^].

It is also worth being aware of the way that the templates actually work behind the scenes; when a template is instantiated it has a working directory. In the example above the catalog:fetch and fetch:template actually copy files into this temporary area. Then the action (not shown) for publishing to gitlab pushes this area as the files into the repo. 

In actuality the `fetch:template` gets *all* the files needed, including the YAML definitions for both the TSSC pipeline and the final deployed application (in various staging projects). RHDH and the template have no knowledge directly of the ArgoCD, Tekton or OpenShift objects, the template is working as a scaffolder. There are actions later in the template to `argocd:create-resources` which use a subdirectory of the scaffolded git repo as the location of the components to instantiate. Like the steps in the template, these are parametrised with content from the template, which allows for the unique creation of the pipelines and ArgoCD applications for this instance. 

It sounds complex but when you realise that the template is just marshaling, scaffolding and deploying files, and the actions of the plugins are doing the work in terms of creating and kicking off the pipelines for build and securing, it becomes simpler to visualise.

The Tekton pipelines themselves are created by the https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#applications[ArgoCD Applications^] and initially contain the opinionated, secure pipelines provided out of the box by RHADS. It must be noted that customers can (and should, if needed) add and alter these pipelines if they have additional security checks and processes to execute as part of the build. In the hands-on lab we will dive into the definition to show where and how this can be changed, but, as said earlier, the majority of end customer security needs will be met by the default security actions provided in the base TSSC pipelines.

=== Tying the loop; hooking code updates to pipeline

So far we have looked at the templating mechanism and how it scaffolds the application; in addition it used ArgoCD to setup a number of effective release gates (_dev_, _pre-prod_, _production_ etc) which can be configured by changing the ArgoCD application definitions and overlays. This, combined with the real-time code updates we discussed earlier (using the DevSpaces plugins) gives the framework for development and staging, but we are missing one vital component.

Tekton (the Pipeline functionality) was designed to allow for the creation of `PipelineRun`s (the actual execution of a Pipeline as opposed to the definition) through a web endpoint, using EventListeners. What we have done with RHADS is provide some out-of-the-box interactions with Git repositories providers (i.e. GitHub, GitLab) which setup webhooks based around the code repositories that are scaffolded as part of the template.

In English, the instantiation of the template not only creates the code repositories to be used and the environments on the OpenShift cluster for the application deployment, it also adds triggers into the code repository to automatically repeat the build pipeline on commits - effectively when the end user commits code to the repository, the pipeline triggers in the appropriate created environments and repeats the entire secure build process.

This guarantees that any code changes to the scaffolded git repos are automatically rebuilt, checked for security issues and receipted and signed using the TPA and TAS components. Again, this automation makes both the developer and the ops persona's lives much easier.


