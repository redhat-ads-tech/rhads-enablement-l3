= TrustyAI

[#introduction]
== Introduction
This section provides a practical implementation with Red Hat OpenShift AI and TrustyAI to achieve security best practices in the AI model development lifecycle.


[#trustyAI]
== Overview
TrustyAI is an open-source community and toolkit for all things responsible AI, with integrations into Red Hat OpenShift AI (RHOAI), OpenDataHub (ODH), Llama-Stack, and KServe.

* A Red Hat team dedicated to maintaining said community and productizing community developments into:
** A component within ODH + RHOAI, for enabling responsible AI workflows within Kubernetes/OpenShift


image::ai-safety/trusty-ai-what.png[width=80%]


== TrustyAI Pillars
image::ai-safety/trustyAI-pillars.png[]

== TrustyAI REST Service
image::ai-safety/trustyAI-rest.png[]

== TrustyAI REST Service: Bias and Drift

Compare the rates at which different input groups receive a specific model outcome.

*Examples:*

* Individuals of race X are 13% more likely to be accepted for loans than individuals of race Y.
* Predicted production defects are 7% more common in the German factory than in the French factory.
* Predictions made in the morning are 15 points lower than those made in the afternoon.


image::ai-safety/trustyAI-bias.png[]


== TrustyAI LLM Evaluation

Perform various link:https://github.com/opendatahub-io/lm-evaluation-harness/blob/main/lm_eval/tasks/README.md[evaluation tasks, window='_blank'] over LLMs to understand and quantify their knowledge, capabilities, and behaviors.

image::ai-safety/trustyai-eval.png[]

* 100+ out-of-the-box  evaluations or tasks
* Create custom tasks via Unitxt

== TrustyAI LLM Guardrails
Moderates the interaction pathways between users and generative models, with:

* Customizable input and output content validators
* Request-time configuration allowing dynamic, per-request guardrailing

image::ai-safety/trusty-ai-guardrails.png[]

*Examples:*

* Prevent users from sending personal information to the models, e.g., don't allow messages with credit card numbers or emails.
* Keep conversations focused to specific topic, such as only talk about subjects that the model scored well on during LM-Eval .
* Detect inappropriate content being generated by the model.
